{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.opt_types import *\n",
    "from lib.utils import *\n",
    "from lib.part_one import f, f_full_data, x_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stochastic gradient methods - 30 Points\n",
    "\n",
    "\n",
    "In this problem, you will implement three different versions of stochastic gradient descent to solve the logistic regression problem on the entire NBA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the stochastic gradient descent methods, we recast our estimation problem [3](#mjx-eqn-eq3) as follows,\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{6}\n",
    "f(\\mathbf{x}) = \\frac{1}{n}\\sum_{i=1}^n\\bigg\\{\\underbrace{\\log(1 + \\exp(- b_i \\mathbf{a}_i^T\\mathbf{x})) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|^2}_{f_i(\\mathbf{x})}\\bigg\\},\n",
    "\\end{equation}\n",
    "\n",
    "where we for notational convenience suppress the dependency on $\\mu$.\n",
    "As we saw in Problem 1, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f_i(\\mathbf{x}) = -b_i \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "\\end{equation}\n",
    "\n",
    "The objective function can be written as a sum of $n$ terms. We augment the `Function` type introduced in previous notebooks to include the following attributes:\n",
    "\n",
    "- To access the gradient of the `j`-th term at a point `x` you can write `f.i_grad(j, x)`.\n",
    "- The number of terms $n$ is stored in the attribute `f.n`. \n",
    "\n",
    "Consider the following stochastic gradient update: At the iteration $k$, pick $i_k\\in\\{1, \\ldots, n\\}$ uniformly at random and define\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{k+1} :=  \\mathbf{x}^k - \\alpha_k\\nabla{f}_{i_k}(\\mathbf{x}^k).\\tag{SGD}\n",
    "\\end{equation}\n",
    "\n",
    "__(a)__ (2 points) Show that $\\nabla f_{i_k}(\\mathbf{x})$ is an unbiased estimation of $\\nabla f(\\mathbf{x})$. Explain why $\\nabla f_{i_k}$ is Lipschitz continuous with $L(f_{i_k}) = \\|\\mathbf{a}_{i_k}\\|^2+ \\mu $. \n",
    "\n",
    "__Hint__: Recall how we computed $L$ in Problem 1. In the following, we will set $L_{\\max} = \\max_{i\\in\\{1, \\ldots, n\\}}L(f_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ (2 points) \n",
    "We can use the standard stochastic gradient descent method [SGD](#mjx-eqn-eqSGD) to solve [6](#mjx-eqn-eq6). \n",
    "Implement `SGD` by completing the following cells with $\\alpha_k =\\frac{0.01}{k}$.\n",
    "\n",
    "**Hint**: For some `N`, the function `np.random.choice(N)` generates a random integer in $\\{0, \\dots, N-1 \\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SGD_state(OptState):\n",
    "    x_k: Vector\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_update(f, state):\n",
    "    x_k, k = state\n",
    "    \n",
    "    i_k = #FILL\n",
    "    \n",
    "    next_x_k = #FILL\n",
    "    \n",
    "    return SGD_state(next_x_k, k+1)\n",
    "\n",
    "def SGD_initialize(f, x_zero):\n",
    "    return SGD_state(x_zero, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = OptAlgorithm(name=\"SGD\", init_state=SGD_initialize, state_update=SGD_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ (6 points) \n",
    "Consider the following stochastic averaging gradient method [SAG](#mjx-eqn-eqSAG) to solve [6](#mjx-eqn-eq6):\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{pick } i_k\\in\\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "\\mathbf{x}^{k+1} := \\mathbf{x}^k - \\frac{\\alpha_k}{n}\\sum_{i=1}^n\\mathbf{v}_i^k,\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{equation}\n",
    "\\mathbf{v}_i^k =\n",
    "\\begin{cases}\n",
    "\\nabla f_i(\\mathbf{x}^k) &\\text{if}\\, i = i_k,\\\\\n",
    "\\mathbf{v}_i^{k-1} &\\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Implement `SAG` by completing the following cells using the step-size \n",
    "$\\alpha_k=\\frac{0.01}{L_{\\max}}$ and $\\mathbf{v}^0=\\mathbf{0}$. (Note that you can access $L_{\\max}$ by writing `f.L_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SAG_state(OptState):\n",
    "    x_k: Vector\n",
    "    v_k: List[Vector]\n",
    "    alpha_k: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAG_update(f, state):\n",
    "    #FILL\n",
    "\n",
    "def SAG_initialize(f, x_zero):\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAG = OptAlgorithm(name=\"SAG\", init_state=SAG_initialize, state_update=SAG_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epochs([SGD, SAG], f, x_zero, 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ (6 points) \n",
    "We can improve the convergence rate of SGD by periodically computing the full gradient. `SVRG` uses the following variance reduction scheme:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{if } k = 0 \\text{ mod } q:\\\\\n",
    "\\quad \\mathbf{z} = \\mathbf{x}^k\\\\\n",
    "\\quad \\tilde{\\mathbf{v}} = \\nabla f(\\mathbf{x}^k)\\\\\n",
    "\\text{Pick } i_k\\in \\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "\\mathbf{d}^k = \\nabla f_{i_k}({\\mathbf{x}}^k) - \\nabla f_{i_k}(\\mathbf{z}) + \\tilde{\\mathbf{v}}\\\\\n",
    "{\\mathbf{x}}^{k+1} := {\\mathbf{x}}^k - \\gamma\\mathbf{d}^k\\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Implement `SVRG` by completing the following cells and fixing the following constant:\n",
    "$\\gamma = 1/L_{\\max}$ and $q = 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SVRG_state(OptState):\n",
    "    x_k: Vector \n",
    "    z: Vector\n",
    "    tilde_v: Vector\n",
    "    q: int\n",
    "    gamma: float\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVRG_update(f, state):\n",
    "    #FILL\n",
    "\n",
    "def SVRG_initialize(f, x_zero):\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVRG = OptAlgorithm(name=\"SVRG\", init_state=SVRG_initialize, state_update=SVRG_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(e)__ (8 points) Another variance reduction method is `SARAH`. The scheme can be described as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{if } k = 0 \\text{ mod } q:\\\\\n",
    "\\quad {\\mathbf{x}}^k = {\\mathbf{z}}\\\\\n",
    "\\quad {\\mathbf{v}}^k = \\nabla f({\\mathbf{z}})\\\\\n",
    "\\quad t \\in \\{0, \\ldots, q-1\\} \\text{ uniformly at random}\\\\\n",
    "\\text{if } k = t \\text{ mod } q:\\\\\n",
    "\\quad {\\mathbf{z}} = {\\mathbf{x}}^{k}\\\\\n",
    "\\text{Pick } i_k\\in \\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "{\\mathbf{v}}^{k+1} = \\nabla f_{i_k}({\\mathbf{x}}^k) - \\nabla f_{i_k}({\\mathbf{x}^{k-1}}) + {\\mathbf{v}}^{k}\\\\\n",
    "{\\mathbf{x}}^{k+1} := {\\mathbf{x}}^k - \\gamma {\\mathbf{v}}^{k+1}\\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Implement `SARAH` by completing the following cells. Pick `q=100` and $\\gamma = \\frac{1}{L_\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SARAH_state(OptState):\n",
    "    x_k: Vector \n",
    "    prev_x_k: Vector\n",
    "    v_k: Vector\n",
    "    z: Vector\n",
    "    q: int\n",
    "    t: int\n",
    "    gamma: float\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARAH_update(f, state):\n",
    "    #FILL\n",
    "\n",
    "def SARAH_initialize(f, x_zero):\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARAH = OptAlgorithm(name=\"SARAH\", init_state=SARAH_initialize, state_update=SARAH_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(f)__ (6 points) Another variance reduction method is `Spider`. The scheme can be described as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{if } k = 0 \\text{ mod } q:\\\\\n",
    "\\quad {\\mathbf{v}}^k = \\nabla f({\\mathbf{x}}^k)\\\\\n",
    "\\text{if } k \\neq 0 \\text{ mod } q:\\\\\n",
    "\\quad \\text{Pick } i_k\\in \\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "\\quad {\\mathbf{v}}^k = \\nabla f_{i_k}({\\mathbf{x}}^k) - \\nabla f_{i_k}({\\mathbf{x}}^{k-1}) + {\\mathbf{v}}^{k-1}\\\\\n",
    "\\eta := \\min \\big(\\frac{1}{\\left \\| {\\mathbf{v}}^{k} \\right \\|_2}, \\frac{1}{\\epsilon}\\big)\\\\\n",
    "{\\mathbf{x}}^{k+1} := {\\mathbf{x}}^k - \\gamma \\eta {\\mathbf{v}}^{k}\\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Implement `Spider` by completing the following cells. Pick `q=100`, $\\epsilon = 0.05$ and $\\gamma = \\frac{1}{L_\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Spider_state(OptState):\n",
    "    x_k: Vector \n",
    "    prev_x_k: Vector\n",
    "    prev_v_k: Vector\n",
    "    q: int\n",
    "    gamma: float\n",
    "    k: int\n",
    "    epsilon: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def Spider_update(f, state):\n",
    "    #FILL\n",
    "\n",
    "def Spider_initialize(f, x_zero):\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spider = OptAlgorithm(name=\"Spider\", init_state=Spider_initialize, state_update=Spider_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epochs([SVRG, SARAH, Spider], f_full_data, x_zero, 50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
