{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code imports\n",
    "from lib.opt_types import *\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image inpainting with proximal methods - 30 points\n",
    "\n",
    "Image in-painting consists of reconstructing the missing parts of an image from a given incomplete image.\n",
    "\n",
    "By exploiting some prior knowledge on the image, it is possible to in-paint images that have a large portion of their pixels missing. In this part of the homework, we are going to study different methods to achieve this goal.\n",
    "\n",
    "We consider a subsampled image $\\mathbf{b} = \\mathbf{P}_{\\Omega} \\mathbf{x}$, where $\\mathbf{P}_{\\Omega} \\in \\mathbb{R}^{n \\times p}$ is an operator that selects only few, $n \\ll p := m^2$, pixels from the vectorized image $\\mathbf{x} \\in \\mathbb{R}^p$. Our goal is to reconstruct the original image $\\mathbf{x}$.\n",
    "\n",
    "### Prior knowledge\n",
    "\n",
    "Image inpainting is impossible without having some prior knowledge on the structure of the true image $\\mathbf{x}$. \n",
    "\n",
    "We will explore and compare the following prior assumptions we can make on the true image: \n",
    ">  **Assumption**: There exists a orthonormal basis $\\mathbf{W} \\in \\mathbb{R}^{p\\times p}$ such that $\\mathbf{x}$ can be sparsely represented in that basis, i.e, $\\mathbf{W} \\mathbf{x}$ is a vector with few non-zero coefficients. Said more formally, this assumption states that there exists $\\mathbf{\\alpha} \\in \\mathbb{R}^p$ with small $\\ell_1$ norm such that $\\mathbf{x} = \\mathbf{W}^\\top\\alpha$. We assume that this basis is known and corresponds to the _wavelet basis_. Under this assumption, the reconstruction problem corresponds to solving the following optimization problem:\n",
    "    $$\n",
    "    \\min_{\\mathbf{\\alpha} \\in \\mathbb{R}^{p}} \\underbrace{ \\frac{1}{2}\\|\\mathbf{b} - \\mathbf{P}_{\\Omega} \\mathbf{W}^T\\mathbf{\\alpha} \\|_2^2}_{f_{\\ell_1}(\\mathbf{\\alpha})} + \\underbrace{\\lambda_{\\ell_1} \\|\\mathbf{\\alpha}\\|_1}_{g_{\\ell_1}(\\mathbf{\\alpha})},\n",
    "    $$\n",
    "    where $\\lambda_{\\ell_1}$ is a coefficient we will need to choose.\n",
    "\n",
    "\n",
    "\n",
    "# PART 1: Optimizing with an $\\ell_1$ norm regularization\n",
    "\n",
    "The optimization problem we are looking to solve have an objective function of the form:\n",
    "$$\n",
    "\t f(\\mathbf{x}) + g(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "These types of objectives are referred to as _composite objectives_ where one term, $f$, is smooth and differentiable and the other term $g$ is non-differentiable.\n",
    "\n",
    "---\n",
    "\n",
    "## Code structure:\n",
    "\n",
    "Recall that we have been working with the `Function` type so far. We will augment this type to represent functions that are not differentiable:\n",
    "\n",
    "- Given a `Function` `g` you can obtain a subgradient at a point `x` by calling `g.subgrad(x)`.\n",
    "\n",
    "Moreover, since we are dealing with _composite_ problems with a an objective function that can be written `f + lambda*g`, we define the `CompositeFunction` type defined as\n",
    "```python\n",
    "@dataclass\n",
    "class CompositeFunction:\n",
    "    f: Function\n",
    "    g: Function\n",
    "```\n",
    "\n",
    "The iterative schemes you will implement will receive a composite function that they can unpack as follows:\n",
    "\n",
    "```python\n",
    "def state_update(composite_function, state):\n",
    "    f, g = composite_function\n",
    "\n",
    "```\n",
    "\n",
    "#### Question 1: (5 point)\n",
    "\n",
    "A first approach to solve a non-smooth optimization problem can be to turn to subgradients. Review slide 42-44 of Lecture 6, and implement `SubG` with $\\alpha_k = \\frac{0.1}{\\sqrt{k}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SubG_state(OptState):\n",
    "    x_k: Vector\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubG_update(composite_function, state):\n",
    "    f, g, = composite_function\n",
    "    #FILL\n",
    "\n",
    "def SubG_initialize(composite_function, x_zero):\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubG = OptAlgorithm(name=\"SubG\", init_state=SubG_initialize, state_update=SubG_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_composite(SubG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Another, more efficient approach, as we saw in Lecture 7, is to minimize such a function by using proximal gradient algorithms, provided that $g$ is _proximable_ (i.e., its proximal operator is efficient to evaluate). We recall the proximal operator of $g$ as the solution to the following convex problem:\n",
    "$$\n",
    "\\mathrm{prox}_g(\\mathbf{z}) := \\mathrm{arg}\\min_{\\mathbf{y}\\in\\mathbb{R}^d}\\{ g(\\mathbf{y}) + \\frac{1}{2}\\Vert\\mathbf{y} - \\mathbf{z}\\Vert_2^2\\}.\n",
    "$$\n",
    "\n",
    "#### Question 2 (2 points)\n",
    "\n",
    "Given $g_{\\ell_1}: \\mathbb{R}^p \\rightarrow \\mathbb{R}, \\; g_{\\ell_1}(\\mathbf{x}) :=  \\|\\mathbf{x}\\|_1$,  show that its proximal function can be written as\n",
    "    $$\n",
    "    \\mathrm{prox}_{\\gamma g_{\\ell_1}}(\\mathbf{z}) = \\max(|\\mathbf{z}|-\\gamma,0) \\circ \\mathrm{sign}(\\mathbf{z}), \\; \\text{for any }\\mathbf{z} \\in \\mathbb{R}^p,\\; \\gamma \\in \\mathbb{R}_+\n",
    "    $$\n",
    "\t\t\twhere the operators $\\max$, $\\mathrm{sign}$ and $\\lvert \\cdot \\lvert$ are applied coordinate-wise to the vector $\\mathbf{z}$ and $\\circ$ stands for $(\\mathbf{x} \\circ \\mathbf{y})_i = x_i y_i$. Such a regularizer imposes sparsity on the solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (1 point)\n",
    " Fill in the function `l1_prox` with the proximal operator of $g_{\\ell_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_prox(gamma, z):\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (4 points)\n",
    "\n",
    "From here on, in order to speed-up the convergence of the optimization, let's assume that the function $f$ has been made to be $\\mu$-strongly convex by the addition of an $\\ell_2$ regularization term to the function $f$, while the $\\ell_1$ term, $g$, has remained unchanged. As such, we must now utilize the strong-convexity versions of the composite optimization algorithms.\n",
    "\n",
    "Using the information in Lecture 7 slide 21 fill in the codes of the method ISTA$_{\\mu}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ISTA_state(OptState):\n",
    "    x_k: Vector\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ISTA_update(composite_function, state):\n",
    "    #FILL\n",
    "\n",
    "def ISTA_initialize(composite_function, x_zero):\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISTA = OptAlgorithm(name=\"ISTA\", init_state= ISTA_initialize, state_update=ISTA_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_composite(ISTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5  (7 points)\n",
    "\n",
    "Using the information in Lecture 7 slide 21 fill in the codes of the method FISTA$_{\\mu}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class FISTA_state(OptState):\n",
    "    x_k: Vector\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def FISTA_update(composite_function, state):\n",
    "    #FILL\n",
    "\n",
    "def FISTA_initialize(composite_function, x_zero):\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FISTA = OptAlgorithm(name=\"FISTA\", init_state= FISTA_initialize, state_update=FISTA_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_composite(FISTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6  (2 points)\n",
    "\n",
    "Compare the convergence rates of the three methods and analyze whether the observed results align with their theoretical bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_methods = [SubG, ISTA, FISTA]\n",
    "compare_composite(list_of_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Application\n",
    "\n",
    "\n",
    "We have now implemented multiple methods that can solve composite optimization problems. We will now apply them to an image inpainting problem.\n",
    "\n",
    "\n",
    "Take a natural image, or better a picture of you, and place it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.inpainting import *\n",
    "\n",
    "image = load(\"lib/alp.jpg\") #FILL IN FILE NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this image in hand, let us subsample it and try to reconstruct the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled = show_subsampled(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we will define the optimization problems we need to solve to perform the reconstruction.\n",
    "\n",
    "- We provide you with a function `P` that acts like the matrix $\\mathbf{P}_{\\Omega}$. That is, given a vector `x`, it returns a subsampled vector `P(x)` that corresponds to $\\mathbf{P}_{\\Omega} \\mathbf{x}$. We also give you `P_T` which acts like $\\mathbf{P}^\\top$.\n",
    "- We provide you with a function `W` and `W_T` that act like the matrix $\\mathbf{W}$ and $\\mathbf{W^\\top}$ respectively. That is, for a vector `x`, `W(x)` and `W_T(x)` return $\\mathbf{W}\\mathbf{x}$ and $\\mathbf{W^\\top}\\mathbf{x}$ respectively.\n",
    "\n",
    "__(a)__ (1 point) Using these provided functions, define the observed variable `b` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.inpainting import P, P_T, W, W_\n",
    "\n",
    "x = image.reshape(-1) #flattened image\n",
    "\n",
    "b = #FILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ (1 point) Now define the function `f_l1` as described earlier in the problem text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = ... # Desired strong convexity of f\n",
    "def f_l1(alpha):\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ (1 point) Write the gradient of $f_{\\ell_1}(\\mathbf{\\alpha})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f_l1(alpha):\n",
    "    #FILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ (1 points) Find the Lipschitz constant of $\\nabla_\\mathbf{\\alpha} f_{\\ell_1}(\\mathbf{\\alpha})$ analytically and fill it in the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lips_grad_f_l1 = #FILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the necessary ingredients to define the smooth part of our composite objective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_l1 = Function(f = f_l1, grad=grad_f_l1, lips_grad = lips_grad_f_l1, strng_cvx=mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the non-smooth term, we define a regularizer built around the `l1` function that re-uses the `l1_prox` function you implemented earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Function(f = lambda x: np.sum(np.abs(x), axis=0), prox=l1_prox)\n",
    "\n",
    "g_l1 = Regularizer(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_objective = CompositeFunction(f=f_l1, g=g_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the objective defined, we provide you with a function with the following signature:\n",
    "```python\n",
    "solve_composite(method: OptAlgorithm, composite_objective: CompositeFunction, lmda: float, max_iterations: int) -> Vector\n",
    "```\n",
    "\n",
    "In other words, the function takes an optimization algorithm `method`, a CompositeFunction `composite_objective`, a regularization parameter `lmda` and a number of iterations `max_iterations` and returns a vector which is the last iterate given by the `method`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.inpainting import solve_composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(e)__ (5 points) Using `solve_composite` reconstruct the subsampled image. Recall that the composite problem solves for $\\mathbf{\\alpha}$ so you need to convert the output back to an image and visualize it.\n",
    "\n",
    "Select a reasonable value for `lmda` or (BONUS) using the function `solve_composite` and the `PSNR` metric, find the best value for `lmda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \n",
    "flat_image_out = \n",
    "image_out = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(true = image, subsampled=subsampled, estimated=image_out)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
